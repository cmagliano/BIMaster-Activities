{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "name": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZSUJk7YPAq-2",
        "pG3w5f0ZGsfN",
        "nMCMVy77Aq-3"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4c26dad8c106fb2eacc6d703993b524774467445",
        "_cell_guid": "258f18fc-e0b2-4a55-8927-51c46077017b",
        "id": "RU48lMd2Aq-e"
      },
      "source": [
        "Introdução\n",
        "====\n",
        "** Processamento de Linguagem Natural ** (PLN) é a tarefa de fazer com que os computadores entendam e produzam linguagens humanas.\n",
        "\n",
        "Author: Claudia Magliano\n",
        "Date: 11/04/2024\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "93f8697704b4f7c9a900bd26c341862d1ef82f05",
        "_cell_guid": "2762e23e-138d-4086-af46-53aa1d7d0bcd",
        "id": "_q5fd-PSAq-k"
      },
      "source": [
        "O que é um Corpus?\n",
        "====\n",
        "\n",
        "Existem muitos corpora (* plural de corpus *) disponíveis em NLTK, vamos começar com um em inglês chamado ** Brown corpus **.\n",
        "\n",
        "Ao usar um novo corpus em NLTK pela primeira vez, faça o download do corpus com a função `nltk.download ()`, por exemplo,\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6a663051c176297596bc3174d10a1f8599247621",
        "_cell_guid": "bd910c35-dc49-4c07-9441-9d16d175580a",
        "id": "96ItC1xjAq-l"
      },
      "source": [
        "Após o download, você pode importá-lo como tal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0E9O4BYJENq",
        "outputId": "a8202230-8f14-4718-d80e-002dc6b65fb2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "637fad23e6bdfd6b8188a7c56318c91b6d3227e2",
        "_cell_guid": "2f1d4c2c-ea22-4494-a650-cfddd3aba49c",
        "collapsed": true,
        "id": "Yr0p8fsKAq-l"
      },
      "source": [
        "from nltk.corpus import brown #Brown é um corpus já criado na biblioteca NLTK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69",
        "_cell_guid": "121fd272-19fd-498f-a3e9-5295d09a2e16",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOGlGciwAq-m",
        "outputId": "daec7cc4-5c5f-4a66-b731-6008eafc7d8c"
      },
      "source": [
        "brown.words() # Returns a list of strings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2841227d2eeba7a7629ffa690647c43dc07bea7f",
        "_cell_guid": "ebf89848-9442-4790-9575-a7f87234eebd",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua6-mMmbAq-m",
        "outputId": "639359bd-4ab3-487d-aa18-1912df44c52f"
      },
      "source": [
        "len(brown.words()) # No. of words in the corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2de0781902fe8662d73bda9381cab9e318cdebf8",
        "_cell_guid": "c3ba66d4-3e26-40fc-94d3-211581fa4c5c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTu7fp9jAq-n",
        "outputId": "7e442304-a4ca-4960-d46b-ad08347550a6"
      },
      "source": [
        "brown.sents() # Returns a list of list of strings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a17b7f140a86b0541acf2796b9146955acd64201",
        "_cell_guid": "68725143-131d-4886-851c-f932c84588aa",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pGDDHJ0Aq-n",
        "outputId": "2647258a-02f9-4d3a-bfd0-a0dd11de2daf"
      },
      "source": [
        "brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qun6dvFYavsg",
        "outputId": "c4f59cd0-99d0-45bd-9133-074c0fdfb416"
      },
      "source": [
        "brown.sents(fileids='ck04') # You can access a specific file with `fileids` argument."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Bishop', 'looked', 'at', 'him', 'coldly', 'and', 'said', ',', '``', 'Take', 'it', 'or', 'leave', 'it', \"''\", '!', '!'], ['Literally', ',', 'there', 'was', 'nothing', 'else', 'to', 'do', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4e2791d121eb8162f2031df6c126bcaa40d812d7",
        "_cell_guid": "972700b2-ee0b-4038-ba23-573287d08fd0",
        "id": "aBrTQg6eAq-n"
      },
      "source": [
        "\n",
        "**Fatos rápidos:**\n",
        "\n",
        "> O Brown Corpus do Inglês Americano Padrão foi o primeiro corpora geral moderno e legível por computador. Foi compilado por W.N. Francis e H. Kucera, Brown University, Providence, RI. O corpus consiste em um milhão de palavras de textos em inglês americano impressos em 1961.\n",
        "\n",
        "(Fonte: [site de lingüística da University of Essex Corpus](  https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))\n",
        "\n",
        ">  Este corpus contém texto de 500 fontes, e as fontes foram categorizadas por gênero, como notícias, editorial e assim por diante ... (para uma lista completa, consulte http://icame.uib.no/brown/bcm- los.html).\n",
        "\n",
        "![](http://)(Source: [NLTK book, Chapter 2.1.3](http://www.nltk.org/book/ch02.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "abcb302ff5d44499870e06f7f86490cf077fea2b",
        "_cell_guid": "1ecd0a80-5a12-444a-b9da-768ffef5133f",
        "id": "9fHjXnZCAq-n"
      },
      "source": [
        "Os dados reais do corpus `brown` são ** empacotados como arquivos de texto bruto **. E você pode encontrar seus IDs com:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8c979bdf698b8f7975b1216083168de0317c8ac5",
        "_cell_guid": "9bb09e9a-574f-4c3d-97eb-2091ce989cab",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgyVIBgtAq-o",
        "outputId": "cb65c35a-808f-438b-8d76-abdb8048720c"
      },
      "source": [
        "len(brown.fileids()) # 500 sources, each file is a source."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b6572ea60d0142090d77a44fabc98ee29943a4bb",
        "_cell_guid": "74143a0e-40e2-4450-9ab4-427686f6ac87",
        "id": "2W_qE7HrAq-o"
      },
      "source": [
        "Você pode acessar os arquivos raw com:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3f73483a3d332c4be88f54115d48beaa6351e090",
        "_cell_guid": "9adcf194-d863-4043-85d0-00743c2786c9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxQkSlxAAq-o",
        "outputId": "241cfc24-df10-4f1a-9928-b189864f7150"
      },
      "source": [
        "print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
            "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
            "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
            "\n",
            "\n",
            "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
            "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f293a3c986c26e82394bd58a68bf2a0843b40f80",
        "_cell_guid": "9c58248d-6e32-496f-843b-1a07e3d4afb8",
        "id": "Yfs4zhG2Aq-p"
      },
      "source": [
        "<br>\n",
        "Você verá que ** cada palavra vem com uma barra e um rótulo ** e, ao contrário do texto normal, vemos que ** pontuações são separadas da palavra que vem antes dela**, e.g.\n",
        "\n",
        "> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
        "\n",
        "<br>\n",
        "\n",
        "e também vemos que ** cada frase é separada por uma nova linha **:\n",
        "\n",
        "> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
        ">\n",
        "> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
        "\n",
        "<br>\n",
        "Isso nos leva ao próximo ponto sobre ** tokenização de frase ** e ** tokenização de palavra **."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2275a8deeace1a0080b4194d70a0e6f751026e2d",
        "_cell_guid": "50b08633-6665-44e1-a989-e8b5fd3f11df",
        "id": "CbOeeac2Aq-p"
      },
      "source": [
        "Tokenização\n",
        "====\n",
        "\n",
        "** Tokenização de frases ** é o processo de * dividir strings em “frases” *\n",
        "\n",
        "** Tokenização de palavras ** é o processo de * dividir “frases” em “palavras” *\n",
        "\n",
        "Vamos brincar com alguns textos interessantes, o corpus `singles.txt` do` webtext`. <br>\n",
        "Eles eram alguns ** anúncios de solteiros ** de http://search.classifieds.news.com.au/\n",
        "\n",
        "Primeiro, baixe os dados com `nltk.download ()`:\n",
        "\n",
        "`` `python\n",
        "nltk.download ('webtext')\n",
        "`` `\n",
        "\n",
        "Então você pode importar com:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2CxiYIMcYB",
        "outputId": "07fd3e31-6ea2-4a29-e3bb-650b7221d659"
      },
      "source": [
        "nltk.download('webtext')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829",
        "_cell_guid": "3636768b-77ad-4f0a-922d-1cedd2a8b11a",
        "collapsed": true,
        "id": "D1BpEtM1Aq-p"
      },
      "source": [
        "from nltk.corpus import webtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f899b6ed306c96f7731f1acc51aaba4256ec59b7",
        "_cell_guid": "6f9fd322-e916-4b70-83c3-87078c66fb15",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oujj4MQdAq-p",
        "outputId": "53ed38a6-bdad-4920-ebe8-032670739a19"
      },
      "source": [
        "webtext.fileids()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['firefox.txt',\n",
              " 'grail.txt',\n",
              " 'overheard.txt',\n",
              " 'pirates.txt',\n",
              " 'singles.txt',\n",
              " 'wine.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "89a84224c596cf23b3498a7d5f3dbbf74428ef9e",
        "_cell_guid": "7518ac61-756f-466b-8294-ca6056994415",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKVi9hEqAq-q",
        "outputId": "131afb2c-7555-4651-8e9e-c8ae4003de50"
      },
      "source": [
        "# Each line is one advertisement.\n",
        "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
        "    if i > 10: # Lets take a look at the first 10 ads.\n",
        "        break\n",
        "    print(str(i) + ':\\t' + line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
            "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
            "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
            "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
            "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
            "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
            "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
            "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
            "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
            "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
            "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c1086676ea2bd10932715c1dfc5ebca5f7765389",
        "_cell_guid": "683d51eb-41e1-41bf-9bd2-3314d435f330",
        "id": "RwoUIlYIAq-q"
      },
      "source": [
        "# Vamos ampliar o candidato nº 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde",
        "_cell_guid": "7b46c556-583a-4af2-95d4-93d4d4b55bd2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6xw4hrRAq-q",
        "outputId": "80329489-60c0-48c0-d633-3f077ff0a8ff"
      },
      "source": [
        "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
        "print(single_no8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef",
        "_cell_guid": "f227e306-1fc8-4d0a-bebc-55014599b588",
        "id": "myJ15a-0Aq-q"
      },
      "source": [
        "\n",
        "# Tokenização de frase\n",
        "<br>\n",
        "Em NLTK, `sent_tokenize ()` a função tokenizer padrão que você pode usar para dividir strings em \"* sentenças *\".\n",
        "<br>\n",
        "\n",
        "Ele está usando o [** Punkt algortihm ** de Kiss e Strunk (2006)] (http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zavo7waM6kT",
        "outputId": "69ca0690-a989-48cc-8314-8308c99d007a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a5430c319a9f9cc66f074405d24271fec49e77c5",
        "_cell_guid": "c49b219d-864f-4353-9535-648592f0d847",
        "collapsed": true,
        "id": "HDjQ6I83Aq-q"
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize #sent_tokenizer é para tokenização de setenças"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01",
        "_cell_guid": "0666bda0-ebec-4f2b-ab92-2158b70e38a2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmIZy5--Aq-r",
        "outputId": "2213a8e3-3eeb-4433-9fb1-0d53d6215aa8"
      },
      "source": [
        "sent_tokenize(single_no8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
              " 'Maybe we could explore new beginnings together?',\n",
              " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
              " 'You WONT be disappointed.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f1198990be58fd7d81cb6516651f98a2716824c3",
        "_cell_guid": "17ab6433-7166-4e87-837a-74cde568f98f",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaBbGXwpAq-r",
        "outputId": "a2a475d9-cc4d-44fe-c84c-ba448cbff6e1"
      },
      "source": [
        "for sent in sent_tokenize(single_no8):\n",
        "    print(word_tokenize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
            "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
            "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
            "['You', 'WONT', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3",
        "_cell_guid": "e11cdead-66ae-4e25-9e68-9130297e0758",
        "id": "DDjugt16Aq-r"
      },
      "source": [
        "# Minúsculas\n",
        "\n",
        "Os CAPS nos textos são UM POUCO irritantes embora SABEMOS que o cara está tentando ENFALAR em algo; P\n",
        "\n",
        "Podemos simplesmente ** colocá-los em minúsculas depois de fazer `sent_tokenize ()` e `word_tokenize ()` **. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "197cae1396cc555d02eb64676471a564e1e7f35a",
        "_cell_guid": "665cff0c-7140-444a-8ea9-ce3e20299464",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS3YYVMIAq-r",
        "outputId": "c214e3cf-1686-4e7f-ad15-99f73ee2d7ac"
      },
      "source": [
        "sent_tokenize(single_no8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
              " 'Maybe we could explore new beginnings together?',\n",
              " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
              " 'You WONT be disappointed.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9f54f59d10281ff66ec36648cff7eda1f13f5ba2",
        "_cell_guid": "42c2c2d9-cb6b-41d4-ac89-46440b13e94c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j4PktYbAq-r",
        "outputId": "adb967b0-33d1-46f5-a0b3-6be4cd8a008a"
      },
      "source": [
        "for sent in sent_tokenize(single_no8):\n",
        "    # It's a little in efficient to loop through each word,\n",
        "    # after but sometimes it helps to get better tokens.\n",
        "    print([word.lower() for word in word_tokenize(sent)])\n",
        "    # Alternatively:\n",
        "    #print(list(map(str.lower, word_tokenize(sent))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
            "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
            "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
            "['you', 'wont', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b90631ac206dc4a6495729b06bcd1fc6415134d7",
        "_cell_guid": "08dea492-753c-4dba-8ac1-90c8dce48aef",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3tWToFhAq-s",
        "outputId": "bff35b40-eb04-4795-be4b-9a30b245c4db"
      },
      "source": [
        "print(word_tokenize(single_no8))  # Treats the whole line as one document."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "df75cb932724a29599f66c6229ba3f324a690181",
        "_cell_guid": "3d1a2d29-ab44-44f7-803a-ac3561368f09",
        "id": "0cL0Kr5TAq-s"
      },
      "source": [
        "Palavras irrelevantes\n",
        "====\n",
        "\n",
        "** Palavras irrelevantes ** são palavras sem conteúdo que têm principalmente apenas funções gramaticais\n",
        "\n",
        "No NLTK, você pode acessá-los da seguinte maneira:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rBFK4TuOXWT",
        "outputId": "a4c1cd15-5802-41aa-ad8c-331990713e89"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "501da9982fc07b1deaae173ed816caa01ac2dd79",
        "_cell_guid": "450cc60b-4e90-491f-9dd3-92ae22fd979a",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEVs1bfkAq-s",
        "outputId": "4d850bdb-602c-4310-b16a-ab13682cb430"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "print(stopwords_en)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GHO0B0ChLm5",
        "outputId": "84e3b4aa-376b-4caa-cf35-3bac90c52412"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_pt = stopwords.words('portuguese')\n",
        "print(stopwords_pt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e613de7ef925754b74334b79788acf4648b9be0b",
        "_cell_guid": "e02fc6ff-713d-4090-b7e2-3132c9415549",
        "id": "znHuMsZJAq-t"
      },
      "source": [
        "# Frequentemente, queremos remover palavras irrelevantes quando queremos manter a \"essência\" do documento / frase.\n",
        "\n",
        "Por exemplo, vamos voltar ao nosso `single_no8`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ceef2a1d450726b705b22698b5b35635c942be38",
        "_cell_guid": "17a72a56-a94e-4d0b-af61-370d5f7b3adb",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEwa94UAAq-t",
        "outputId": "bbe32889-c1cb-43f6-c179-73a4a12f8d58"
      },
      "source": [
        "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
        "# Tokenize and lowercase\n",
        "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\n",
        "print(single_no8_tokenized_lowered)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7232e7aacc09962b8b010f6463ff14566471f541",
        "_cell_guid": "ea203327-c900-4d54-a1cd-fb3d94a9f021",
        "id": "Mo9AoHdNAq-t"
      },
      "source": [
        "# Vamos tentar remover as palavras irrelevantes usando a lista de palavras irrelevantes em inglês no NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3b691c536a5487f33b92eceb94b34b8d12cac22f",
        "_cell_guid": "b97fa9b8-6366-40b6-9b0f-05a15fc3b93f",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3YE1MaRAq-t",
        "outputId": "3c812737-fc2c-4dbe-88e2-fd5d1f93ce5d"
      },
      "source": [
        "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
        "\n",
        "# List comprehension.\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdlvW9JDiVaV",
        "outputId": "f3d27db7-96a4-481b-a970-53c70397df1b"
      },
      "source": [
        "print(single_no8_tokenized_lowered)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d99001efcddcd7a080d76f6064ce309e41542f86",
        "_cell_guid": "a6ebff46-17a3-47d0-bf1f-47461ee484a4",
        "collapsed": true,
        "id": "JOzP_SUJAq-t"
      },
      "source": [
        "# Freqüentemente, queremos remover as pontuações dos documentos também.\n",
        "\n",
        "Como o Python vem com \"baterias incluídas\", temos string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "991965258d7aeffa5f55420fdf4e6f4a83dbfd9e",
        "_cell_guid": "fd82dfe3-2221-4c0a-9d0d-1b14a5349b91",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRnLvsBAq-t",
        "outputId": "e88276aa-098a-4db0-dcbf-bb99b527e07c"
      },
      "source": [
        "from string import punctuation\n",
        "# It's a string so we have to them into a set type\n",
        "print('From string.punctuation:', type(punctuation), punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "78edf4a03bd59a7753ba5a0d9a238df439711516",
        "_cell_guid": "c83a6e53-e7f4-43c9-a0b7-522ca1e75e55",
        "id": "UB68YR3BAq-u"
      },
      "source": [
        "# Combinando a pontuação com as palavras irrelevantes de NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "97c96c2b0df9d8cd1dc6c6cf6619ed7f09c3072c",
        "_cell_guid": "ffafed92-9c4c-4fc1-aa7b-89645edf4b8c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgjzaA4BAq-u",
        "outputId": "ae80ca0c-d199-4f5f-a413-e1e56bdb3fc4"
      },
      "source": [
        "stopwords_en_withpunct = stopwords_en.union(set(punctuation)) #Unificando palavras e pontuações irrelevantes\n",
        "print(stopwords_en_withpunct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'where', 'him', 'through', '<', 'off', 'of', \"weren't\", '[', '}', 'y', 'which', \"haven't\", 'will', 'have', 'to', 'there', 'few', 'own', '\\\\', 'its', 'but', '!', '&', 'i', \"she's\", 'that', 'a', 'some', 'here', 'before', 'as', \"you'll\", '|', 'my', \"won't\", 'them', 'what', 'she', 'you', 'down', '~', 'did', 'wasn', \"that'll\", 'no', \"shan't\", 'himself', 'or', 'd', \"doesn't\", 'been', 'same', 'don', 'most', \"shouldn't\", 'just', 'from', 'wouldn', 'her', '=', '\"', ';', \"wasn't\", 'won', \"isn't\", 'these', 'any', 'against', 'haven', '-', 'now', 'yourself', 'hadn', ']', 'so', 'such', 'then', 'if', 'further', 'can', 'ain', 'they', 'those', 'it', 'o', 'needn', 'when', 'his', 'about', \"couldn't\", \"should've\", \"hadn't\", '@', '%', '*', 'an', 'does', 'for', 'than', 'doesn', 'weren', 'are', 'myself', 'below', 'only', 'up', 'didn', \"you've\", '`', 'who', 'our', \"hasn't\", 'should', 'once', 've', 'couldn', 'had', '(', 'during', 'all', 'over', 'more', 'me', 'herself', 'in', 'yourselves', 's', 're', 'ourselves', 'theirs', 'again', 'whom', 'out', 'their', 'm', '#', '_', 'yours', 'itself', 'shouldn', 'both', \"you'd\", '+', 'being', '$', 'very', \"didn't\", \"it's\", 'while', '/', 'nor', 'with', 'too', 'doing', ')', 'ma', 'other', '^', ',', 'were', '.', 'the', '>', \"'\", 'shan', 'he', 'until', 'each', 'your', 'and', 'because', 't', 'how', 'mightn', \"needn't\", \"wouldn't\", 'am', '?', 'be', 'between', 'by', 'isn', 'not', 'on', 'was', 'themselves', 'after', 'under', 'why', 'mustn', \"mustn't\", \"you're\", ':', 'at', 'into', 'hers', \"don't\", 'this', 'is', \"mightn't\", 'we', 'll', '{', 'hasn', 'has', 'having', 'do', 'above', 'ours', \"aren't\", 'aren'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e6b6acafbca2e1c8f88fbda2313da9d9b4961cf4",
        "_cell_guid": "230db9d8-bb06-4e48-aad9-4c4609448c4a",
        "id": "6oFDO3iIAq-u"
      },
      "source": [
        "# Removendo palavras irrelevantes com pontuações de Single no. 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c37dbd0c8a7658dd20b2266276baf0e162204576",
        "_cell_guid": "8cc6b68c-29b1-48b2-baa1-bd3f9bc4e21f",
        "collapsed": true,
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GxamiZeAq-u",
        "outputId": "405a854b-1ac5-4d7f-9519-005a87b7f9dd"
      },
      "source": [
        "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "027cd197d81df7b34048c520a84270c74781a6f0",
        "_cell_guid": "31f5c735-c260-4a4d-870e-721ccca5cb8b",
        "id": "YSlaK7xtAq-u"
      },
      "source": [
        "# Usando uma lista mais forte / mais longa de palavras irrelevantes\n",
        "\n",
        "Da saída anterior, ainda temos verbos de modelo pendentes (ou seja, 'poderia', 'não', etc.).\n",
        "\n",
        "Podemos combinar as palavras irrelevantes que temos no NLTK com outras listas de palavras irrelevantes que encontramos online.\n",
        "\n",
        "Pessoalmente, gosto de usar `stopword-json` porque tem stopwrds em 50 idiomas =) <br>\n",
        "https://github.com/6/stopwords-json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "376d2a04068b557bc5080353a998aa52c199dccb",
        "_cell_guid": "adf1ed39-3116-46f0-92c0-6dfddcd904a0",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2q5Z2d9Aq-u",
        "outputId": "13b28b2c-dd80-4a5e-a930-b7e5d26f4300"
      },
      "source": [
        "# Stopwords from stopwords-json\n",
        "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
        "stopwords_json_en = set(stopwords_json['en'])\n",
        "stopwords_nltk_en = set(stopwords.words('english'))\n",
        "stopwords_punct = set(punctuation)\n",
        "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
        "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
        "\n",
        "# Remove the stopwords from `single_no8`.\n",
        "print('With combined stopwords:')\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With combined stopwords:\n",
            "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "84c1307d0bbc465925ef9932eddcbcdfd95a17ca",
        "_cell_guid": "273f7eb4-77ff-4cb9-abf0-4254122fa2b7",
        "id": "vyuwI4cFAq-y"
      },
      "source": [
        "\n",
        "# Stemming and Lemmatization\n",
        "\n",
        "Frequentemente, queremos mapear as diferentes formas da mesma palavra para a mesma palavra raiz, por exemplo, \"caminha\", \"caminha\", \"caminhou\" deve ser o mesmo que \"caminhar\".\n",
        "\n",
        "O processo de lematização e lematização são regras regex escritas à mão para localizar a palavra raiz.\n",
        "\n",
        "  - ** Stemming **: tentativa de encurtar uma palavra com regras simples de regex\n",
        "\n",
        "  - ** Lemmatização **: Tentando encontrar a palavra raiz com regras linguísticas (com o uso de regexes)\n",
        "\n",
        "(Veja também: [Stemmers vs Lemmatizers] (https://stackoverflow.com/q/17317418/610569) question on StackOverflow)\n",
        "\n",
        "Existem vários lematizadores e um lematizador em NLTK, sendo o mais comum:\n",
        "\n",
        "  - ** Porter Stemmer ** de [Porter (1980)] (https://tartarus.org/martin/PorterStemmer/index.html)\n",
        "  - ** Wordnet Lemmatizer ** (porta do Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3dd75f548e7363aa69b9da126a886f7299e052be",
        "_cell_guid": "f41dab63-6133-4a51-9eb6-f87414b30c30",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pajk-3YeAq-1",
        "outputId": "cef3e01b-c50b-43c6-95ed-847278edff37"
      },
      "source": [
        "from nltk.stem import PorterStemmer #Muito radical: deixa apenas a raiz da palavra. O stemming não é dependente de linguagem, a lemetização é\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(porter.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "walk\n",
            "walk\n",
            "walk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34WxugEKP7FN",
        "outputId": "f46b5f58-5945-4230-fe86-cfe472250802"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "31f49462e63529ac6d25b16389ef8494343b4de4",
        "_cell_guid": "edf939e2-bc38-4c50-92db-fd8d958ad17d",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOLt_O6FAq-1",
        "outputId": "8ecc5cb0-ecb8-440a-b28e-3be0598898b7"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(wnl.lemmatize(word))\n",
        "\n",
        "#Para que  lemetizador funcione bem, é necessário fazermos as tags of speech anteriormente para classificarmos verbos, advérbios, etc..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "walking\n",
            "walk\n",
            "walked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d2fa9f729a413a1417461683aa667f55a112e4e2",
        "_cell_guid": "fe9ffb81-f53a-4c58-97b6-86e0d79c51ca",
        "id": "DBEPb4UwAq-1"
      },
      "source": [
        "#Peguei vocês! O lematizador é realmente muito complicado, ele precisa de tags Parts of Speech (POS).\n",
        "Não cobriremos o que é PDV hoje, então vou apenas mostrar como \"chicotear\" o lematizador para fazer o que você precisa.\n",
        "\n",
        "Por padrão, a função WordNetLemmatizer.lemmatize () assumirá que a palavra é um substantivo se não houver uma tag POS explícita na entrada.\n",
        "\n",
        "Primeiro, você precisa da função pos_tag para marcar uma frase e, usando a tag, convertê-la em conjuntos de tags WordNet e, em seguida, colocá-la no WordNetLemmatizer.\n",
        "\n",
        "Observação: a lematização não funcionará realmente com palavras isoladas sem contexto ou conhecimento de sua tag POS (ou seja, precisamos saber se a palavra é um substantivo, verbo, adjetivo, advérbio)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBWgzRETQuCa",
        "outputId": "f873b26d-4add-48b8-9792-904ad3ef30e0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0813197200488bb07b8e23f5281ae324b323e981",
        "_cell_guid": "0fa94379-f1b2-4228-b30c-001d2895b993",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5E75QM1Aq-1",
        "outputId": "41aade06-5a26-4f6e-e6b0-d35cc711a756"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' # if mapping isn't found, fall back to Noun.\n",
        "\n",
        "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
        "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
        "# so we need to get the tag from the 2nd element.\n",
        "\n",
        "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
        "print(walking_tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e59549b59e571e2368d7c08c80250a854259d1d1",
        "_cell_guid": "6a826e5e-1c16-46ad-ac52-916a2f4e5c41",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEsPPY3YAq-1",
        "outputId": "528b6e59-8fdd-4c64-ad7e-0cb9c567990d"
      },
      "source": [
        "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "034d83608a867c28c06b86c9dc129b2f88faf326",
        "_cell_guid": "46b8011c-beba-4e69-8305-f5b98659e679",
        "id": "LFFKiNjvAq-1"
      },
      "source": [
        "# Agora, vamos criar uma nova função de lematização para sentenças, dado o que aprendemos acima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4f7803ff5cbc3f6c0dc97ce9c353a0bec39a5696",
        "_cell_guid": "6fe6901c-88ca-438f-9a9a-b1f28128ffc4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygvWGJNBAq-2",
        "outputId": "2cba02ff-36b6-446c-9678-ffc86ccdf954"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "\n",
        "def lemmatize_sent(text):\n",
        "    # Text input is string, returns lowercased strings.\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
        "            for word, tag in pos_tag(word_tokenize(text))]\n",
        "\n",
        "lemmatize_sent('He is walking to school')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6ad66c3cabdf30287f2f6b0ea5ccbfdcd37cf927",
        "_cell_guid": "346bab96-cf6c-4a91-b27d-bfced577ea15",
        "id": "4xIWIBduAq-2"
      },
      "source": [
        "# Vamos tentar o `lemmatize_sent ()` e remover as palavras de interrupção do Single no. 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e591e6f136501cee25961fb1df0e68f1eb676b45",
        "_cell_guid": "926f7ec4-12c7-4d8d-8f2b-ef66b0b3a5a4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8SP65erAq-2",
        "outputId": "30b78289-904e-4fb8-ee96-bec4300853a7"
      },
      "source": [
        "print('Original Single no. 8:')\n",
        "print(single_no8, '\\n')\n",
        "print('Lemmatized and removed stopwords:')\n",
        "print([word for word in lemmatize_sent(single_no8)\n",
        "       if word not in stoplist_combined\n",
        "       and not word.isdigit() ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Single no. 8:\n",
            "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
            "\n",
            "Lemmatized and removed stopwords:\n",
            "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "56629a24758169967e39e8a93576195fb7c27ee8",
        "_cell_guid": "9c245e23-b526-4d86-9082-912c1aa87255",
        "id": "jKxDMDKVAq-2"
      },
      "source": [
        "# Combinando o que sabemos sobre a remoção de palavras irrelevantes e lematização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3145c9a5568eb2ed075eddf5a617d20b2286f012",
        "_cell_guid": "358ca69f-e7b2-4b93-ad09-5cf1b0a7143a",
        "collapsed": true,
        "id": "aXFgeNQoAq-2"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Input: str, i.e. document/sentence\n",
        "    # Output: list(str) , i.e. list of lemmas\n",
        "    return [word for word in lemmatize_sent(text)\n",
        "            if word not in stoplist_combined\n",
        "            and not word.isdigit()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "099da61cd361f9128632758422960862742b917f",
        "_cell_guid": "51ffeed8-ea00-479e-bc92-422f4e0bba0e",
        "id": "ZSUJk7YPAq-2"
      },
      "source": [
        "# Nota tangencial sobre lematização\n",
        "\n",
        "Em inglês, uma palavra raiz / lema pode se manifestar em diferentes formas.\n",
        "\n",
        "| <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> | <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"><img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> |\n",
        "|:-------------:|:-------------:|\n",
        "| 1 cat  | 2 cats  |\n",
        "| 1 cat  | 2 cats  |\n",
        "\n",
        "Por exemplo, usamos “gato” para nos referirmos a um único “gato” e anexamos um sufixo “-s” para nos referirmos a mais de um gato, por exemplo, \"dois gatos\".\n",
        "\n",
        "| <img src=\"https://68.media.tumblr.com/b0755247c8f32f79413d34b0410ccff1/tumblr_o3q8wlGi9v1u9ia8fo1_500.gif\" align=\"left\" height=\"200\" width=\"400\"> |\n",
        "|:-------------:|\n",
        "| cats walk / cats (are) walking |\n",
        "\n",
        "<!-- | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/ModelsCatwalk.jpg/440px-ModelsCatwalk.jpg\" align=\"left\" height=\"200\" width=\"400\"> |\n",
        "|:-------------:|\n",
        "| cat walk(s) / catwalk(s) |  -->\n",
        "\n",
        "\n",
        "Outro exemplo, a palavra “andar” tem diferentes formas, por ex. “Caminhando” e “caminhado” indicam o tempo e / ou progresso do movimento de caminhada. <! - ~~ Além disso, “caminhar” também pode se referir ao ato de caminhar, que é diferente do movimento de caminhar, por exemplo, \"João foi dar um passeio\" (ato de caminhar) vs \"João queria caminhar até o parque\" (a ação / movimento de caminhada). ~~ -> Chamamos essas palavras raiz de *** tipos de palavras *** (por exemplo, “gato” e “caminhar”) e suas diferentes formas como *** palavras-chave *** (por exemplo, “gatos”, “caminhar”, “caminhar”, “caminhar”, “caminhar”).\n",
        "\n",
        "Os lingüistas distinguem ainda mais as palavras entre seus lemas ou famílias de palavras. Um lema se refere à palavra raiz canônica usada como entrada do dicionário. Uma família de palavras se refere a um grupo de lemas derivados de uma única palavra raiz. Mesmo que \"walkable\" seja uma entrada separada em um dicionário de \"walk\", \"walkable\" pode ser agrupado sob a palavra família de \"walk\" junto com \"walking, walk, walk\".\n",
        "\n",
        "A distinção é sutil, embora os linguistas se esforcem para argumentar sobre o que conta como um tipo, token, lemas ou família de palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG3w5f0ZGsfN"
      },
      "source": [
        "#vamos a uns slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3f40b6cb3f60bc2bbfeebd27de0802d0bf7849cf",
        "_cell_guid": "9b00681d-5757-4547-828f-5767bfe701bd",
        "id": "muNmyk68Aq-3"
      },
      "source": [
        "#** Vector ** é uma matriz de números\n",
        "\n",
        "** Vector Space Model ** conceitua a linguagem como um monte de números\n",
        "\n",
        "** Bag-of-Words (BoW) **: Contagem de cada documento / frase como um vetor de números, com cada número representando a contagem de uma palavra no corpus\n",
        "\n",
        "Para contar, podemos usar o Python `coleções.Contador`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e5527ef7e753e6b3981c9909b50fcf39a428a85c",
        "_cell_guid": "37963ad4-b911-4bfc-b723-f6b7927a1eeb",
        "collapsed": true,
        "id": "xNE4iaa8Aq-3"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "# Lemmatize and remove stopwords\n",
        "processed_sent1 = preprocess_text(sent1)\n",
        "processed_sent2 = preprocess_text(sent2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cfb48c07796e6b3fbe20378dcef1f11e4880b1f9",
        "_cell_guid": "882d59c1-e3bf-4e72-84e2-858490ec66b7",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8BXQEwDAq-3",
        "outputId": "84d0f895-be60-4b25-e079-e3be32e05c31"
      },
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent1)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed sentence:\n",
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "\n",
            "Word counts:\n",
            "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4e9bd5f5852015cf8f004d1f7a01274b76cb033d",
        "_cell_guid": "75e94392-cf5c-40f5-bcd7-349e4861fab7",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCDMMZIhAq-3",
        "outputId": "5bd91b8b-0837-4d4b-d8af-f8bcbe7a0836"
      },
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent2)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed sentence:\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Word counts:\n",
            "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'fox': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "25205be78a242dcb9b8bd653cd98244ce73ba69b",
        "_cell_guid": "e51b4c01-2712-4ec9-852b-133716681b53",
        "id": "nMCMVy77Aq-3"
      },
      "source": [
        "# Vetorização\n",
        "\n",
        "Vamos colocar as palavras e contagens em uma boa tabela:\n",
        "\n",
        "| | brown | quick | fox | jump | lazy | dog | mr |\n",
        "|:---- |:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
        "| Sent1 | 2 | 1 | 1 | 1 | 1 | 1 | 0 |  \n",
        "| Sent2 | 1 | 0 | 1 | 1 | 1 | 0 | 1 |\n",
        "\n",
        "\n",
        "Se fixarmos as posições do vocabulário, ou seja,\n",
        "\n",
        "```\n",
        "[brown, quick, fox, jump, lazy, dog, mr]\n",
        "```\n",
        "\n",
        "e fazemos as contagens para cada palavra em cada frase, obtemos os vetores das frases (ou seja, lista de números para representar cada frase):\n",
        "\n",
        "```\n",
        "sent1 = [2,1,1,1,1,1,0]\n",
        "sent2 = [1,0,1,1,1,0,1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5c268c3e117c595b2835cdfd7c5abe604f35e0fa",
        "_cell_guid": "64860ccd-4cf4-46dd-89d2-3929d9b7d539",
        "id": "Ye_VmB-mAq-3"
      },
      "source": [
        "# Vetorização com sklearn\n",
        "\n",
        "No `scikit-learn`, existem funções pré-construídas para fazer o pré-processamento e a vetorização que estamos fazendo usando o objeto` CountVectorizer`.\n",
        "\n",
        "Será o objeto que contém o vocabulário (ou seja, a primeira linha de nossa tabela acima) e tem a função de converter qualquer frase nos vetores de contagem que vemos acima.\n",
        "\n",
        "A entrada que `CountVectorizer` é um arquivo de texto, então temos que fazer alguns hacks para deixá-lo aceitar as saídas de string.\n",
        "\n",
        "Podemos \"fingir para torná-lo\" usando `io.StringIO`, onde podemos converter qualquer string para funcionar como um arquivo, por exemplo,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3834e62e0c3879b37f94e1924e80534a5899fbfd",
        "_cell_guid": "5f92d415-5016-4405-b797-d98df2393a89",
        "collapsed": true,
        "id": "YSEy2Ef4Aq-3"
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Create the vectorizer\n",
        "    count_vect = CountVectorizer()\n",
        "    count_vect.fit_transform(fin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "36407729e4ed2636089293545a5f2d030a4d60e8",
        "_cell_guid": "a8a7db77-9578-4f58-8c09-8cde03f6a865",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafVLGZ9Aq-4",
        "outputId": "403c0c04-2445-4609-e67a-c48ec23160bc"
      },
      "source": [
        "# We can check the vocabulary in our vectorizer\n",
        "# It's a dictionary where the words are the keys and\n",
        "# The values are the IDs given to each word.\n",
        "count_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0,\n",
              " 'dog': 1,\n",
              " 'fox': 2,\n",
              " 'jumps': 3,\n",
              " 'lazy': 4,\n",
              " 'mr': 5,\n",
              " 'over': 6,\n",
              " 'quick': 7,\n",
              " 'the': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "07b29c69132ed41c5da7f869f098694374b42cda",
        "_cell_guid": "2249db3e-0b22-4f63-be8e-b19c951a4ea2",
        "id": "zQO8Tb7NAq-4"
      },
      "source": [
        " (Wait a minute)\n",
        "\n",
        "Eu não disse ao vetorizador para remover a pontuação, tokenizar e minúsculas, como eles fizeram isso?\n",
        "\n",
        "Além disso, está no vocabulário, é uma palavra de ordem, queremos que desapareça ...\n",
        "E os saltos não são interrompidos ou lematizados!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e6393e0f95e0b0dbd92f64f22883331f9eaac919",
        "_cell_guid": "035f1601-9e72-4d85-929e-44d8447d7253",
        "id": "IBBCB0NwAq-4"
      },
      "source": [
        "vamos ver a documentação de [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in `sklearn`, a gentre ve:\n",
        "\n",
        "\n",
        "```python\n",
        "CountVectorizer(\n",
        "    input=’content’, encoding=’utf-8’,\n",
        "    decode_error=’strict’, strip_accents=None,\n",
        "    lowercase=True, preprocessor=None,\n",
        "    tokenizer=None, stop_words=None,\n",
        "    token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1),\n",
        "    analyzer=’word’, max_df=1.0, min_df=1,\n",
        "    max_features=None, vocabulary=None,\n",
        "    binary=False, dtype=<class ‘numpy.int64’>)[source]\n",
        "```\n",
        "\n",
        "E mais especificamente:\n",
        "\n",
        "> ** analisador **: string, {‘word’, ‘char’, ‘char_wb’} ou chamável\n",
        ">\n",
        "> Se o recurso deve ser composto de n-gramas de palavras ou caracteres. A opção ‘char_wb’ cria caracteres n-gramas apenas do texto dentro dos limites da palavra; n-gramas nas bordas das palavras são preenchidas com espaço.\n",
        "> Se um chamável for passado, ele será usado para extrair a sequência de recursos da entrada bruta e não processada.\n",
        "\n",
        "\n",
        "> ** pré-processador **: chamável ou Nenhum (padrão)\n",
        ">\n",
        "> Substituir o estágio de pré-processamento (transformação de string), preservando as etapas de geração de tokenização e n-gramas.\n",
        "\n",
        "> ** tokenizer **: chamável ou Nenhum (padrão)\n",
        ">\n",
        "> Substitua a etapa de tokenização da string enquanto preserva as etapas de pré-processamento e geração de n-gramas. Aplica-se apenas se o analisador == 'palavra'.\n",
        "\n",
        "> ** stop_words **: string {‘english’}, list ou None (default)\n",
        ">\n",
        "> Se for \"inglês\", uma lista de palavras de interrupção integrada para o inglês é usada.\n",
        "> Se for uma lista, presume-se que essa lista contenha palavras de parada, as quais serão removidas dos tokens resultantes. Aplica-se apenas se o analisador == 'palavra'.\n",
        "Se nenhum, nenhuma palavra de parada será usada.\n",
        "\n",
        "> ** minúsculas **: booleano, verdadeiro por padrão\n",
        ">\n",
        "> Converta todos os caracteres em minúsculas antes de tokenizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "11d7d8b6406c5d56259d72c48f97747e119885b7",
        "_cell_guid": "9eb50e4d-4d52-47a4-9607-fd33fe3775d1",
        "id": "IBTvdqK3Aq-4"
      },
      "source": [
        "# Então, podemos substituir esses argumentos com as funções que aprendemos antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4875ebac4a09758236313d7ffa17fb015faac926",
        "_cell_guid": "e9c00de2-ab64-4a35-a790-485a64b43dd3",
        "id": "E3hDMB6uAq-4"
      },
      "source": [
        "We can **override the tokenizer and stop_words**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f91435b9a2d476d45d9146ceb204f64e586133c1",
        "_cell_guid": "25630055-8410-46fe-b08c-5fc7202bf1ab",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUE10tNuAq-4",
        "outputId": "3a03fe5c-cb07-4daa-871f-4e3025cf8e2d"
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
        "                                 tokenizer=word_tokenize)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0, 'dog': 1, 'fox': 2, 'jumps': 3, 'lazy': 4, 'mr': 5, 'quick': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljXsgJjF04AP",
        "outputId": "c33b480f-8711-49c3-aa00-fa388b6ddd51"
      },
      "source": [
        "preprocess_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.preprocess_text>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e23d630dd87abeae6ed1b37d64c2fe5223da7088",
        "_cell_guid": "5f29251e-1074-4c04-a50c-d56d4ca3a97e",
        "id": "TWnchimlAq-5"
      },
      "source": [
        "Ou apenas ** substitua o analisador ** totalmente com nosso texto de pré-processamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fd10c8df7c870641807211b97140900e570c46af",
        "_cell_guid": "40813b12-f701-4bca-a2e3-c16b928dd32e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AodvAx5yAq-5",
        "outputId": "c70be8c0-762a-4006-f575-aa1a7fcbf29c"
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0, 'dog': 1, 'fox': 2, 'jump': 3, 'lazy': 4, 'mr': 5, 'quick': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4024585e83698a0c454b6ff332de4a32d8bfeb8b",
        "_cell_guid": "ab69e27d-1d13-47f0-a547-9e234292d166",
        "id": "YSxOHJULAq-5"
      },
      "source": [
        "# Para vetorizar quaisquer novas sentenças, usamos `CountVectorizer.transform ()`\n",
        "\n",
        "A função retornará uma matriz esparsa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e18dbaa0882825ffc4b1b746de13eb4d8381984d",
        "_cell_guid": "92d41c12-7f75-406e-80ec-66a2977a79a8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DCq0-aGAq-5",
        "outputId": "26326252-b6a9-4183-a9ad-3bd99873cc42"
      },
      "source": [
        "count_vect.transform([sent1, sent2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 11 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8815c6acca7b396407004eaf858a957576c06673",
        "_cell_guid": "81e138ab-0b0f-4c67-abe9-b34c15cb7e8d",
        "id": "tl2OEnDXAq-5"
      },
      "source": [
        "# Para visualizar a matriz, você pode enviá-la para um array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ec34de51827ed00a2cdb39c000a03bb85e582713",
        "_cell_guid": "4e4029f4-1440-4ebd-b239-c365e735e21d",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrxEGIjRAq-5",
        "outputId": "560f8e7a-9f5b-4747-945b-aa8660ef27a7"
      },
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "# Print the words sorted by their index\n",
        "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
        "\n",
        "print(preprocess_text(sent1))\n",
        "print(preprocess_text(sent2))\n",
        "print()\n",
        "print('Vocab:', words_sorted_by_index)\n",
        "print()\n",
        "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Vocab: ('brown', 'dog', 'fox', 'jump', 'lazy', 'mr', 'quick')\n",
            "\n",
            "Matrix/Vectors:\n",
            " [[2 1 1 1 1 0 1]\n",
            " [1 0 1 1 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42IRRN5GfP9w"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}